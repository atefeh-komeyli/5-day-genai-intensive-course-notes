


# üí° DAY 1: Deep Dive into Large Language Models (LLMs)


## üß† Training Process

Training LLMs typically involves three stages:

- **Pre-training**: The model is trained on massive amounts of raw, unlabeled text to learn general language representations.
- **Fine-tuning**: This stage adapts the pre-trained model to specific tasks using smaller labeled datasets.
- **Reinforcement Learning from Human Feedback (RLHF)**: Human preferences are used to train a reward model, which then guides further fine-tuning.

To reduce compute requirements during fine-tuning, **Parameter-Efficient Fine-Tuning (PEFT)** techniques are used, such as:

- **Adapters**: Small trainable layers added to the network.
- **LoRA / QLoRA**: Low-Rank Adaptation techniques that update only a fraction of the weights.
- **Soft Prompting**: Using trainable embeddings instead of modifying full model weights.

---

## ‚úçÔ∏è Prompt Engineering Techniques

Prompt engineering is crucial for getting high-quality outputs from LLMs. Techniques include:

- **Zero-shot prompting**: Ask the model a question without any examples.
- **One-shot / few-shot prompting**: Provide one or more examples to guide the response.
- **Chain-of-Thought (CoT)**: Ask the model to show its reasoning before answering.
- **Self-consistency**: Sample multiple reasoning paths and pick the most consistent answer.
- **Tree of Thoughts (ToT)**: Explore multiple reasoning paths in parallel.
- **ReAct**: Combine reasoning with the ability to take actions.
- **Automatic Prompt Engineering (APE)**: Generate and evaluate prompts automatically.
- **System Prompting**: Define a persistent context or behavior.
- **Role Prompting**: Assign a persona to shape tone/style.
- **Contextual Prompting**: Provide detailed background for relevance.
- **Step-back Prompting**: Start with a broader question to prime knowledge.

---

## ‚öôÔ∏è Sampling and Generation Techniques

The method used to generate text greatly affects its quality:

- **Temperature**: Controls randomness; lower = more deterministic.
- **Top-K sampling**: Limits token selection to top K probabilities.
- **Top-P (nucleus) sampling**: Picks from the smallest set of tokens whose total probability exceeds a threshold.
- **Greedy search**: Picks the most probable token at each step.
- **Random sampling**: Adds diversity but may reduce coherence.
- **Best-of-n sampling**: Generates multiple completions and picks the best.

---

## üöÄ Model Evolution

- **GPT-1**: Introduced generative pre-training + fine-tuning.
- **BERT**: Encoder-only, focused on understanding tasks.
- **GPT-2**: Larger model with strong zero-shot performance.
- **GPT-3**: 175B parameters; great few-shot generalization.
- **InstructGPT**: GPT-3 + human feedback for instruction-following.
- **GPT-3.5**: Improved reasoning and efficiency.
- **GPT-4**: Multi-modal with stronger reasoning and tool use.
- **LaMDA**: Dialogue-optimized model from Google.
- **Gopher**: DeepMind‚Äôs language model.
- **PaLM & PaLM 2**: Google‚Äôs models for reasoning, multilingual tasks, and code.
- **Gemini**: Multi-modal successor to PaLM with TPU optimization.
- **Gemma**: Lightweight, open-source model family (text-only).
- **LLaMA 3**: Meta‚Äôs powerful open-source LLMs.
- **Mistral**: Efficient, high-performance open models.
- **O1 Models**: Transparent, fine-tunable open-source models.
- **DeepSeek**: Introduced Group Relative Policy in RL.

---

## ‚ö° Inference Optimization

Techniques to improve speed and efficiency:

- **Quantization**: Reduces weight precision to save memory.
- **Quantization-Aware Training (QAT)**: Maintains accuracy while quantizing during training.
- **Distillation**: Trains smaller models to mimic larger ones.
- **Flash Attention**: Optimized attention for faster inference.
- **Prefix Caching**: Reuses previous token computations.
- **Speculative Decoding**: Drafts tokens with a small model, validates with a larger one.

---

## üìè Model Evaluation

Evaluation varies by use case and combines auto metrics + human feedback:

- **Accuracy**: Factual correctness.
- **Helpfulness**: Matches user intent.
- **Creativity**: Innovative responses.
- **BLEU / ROUGE**: Metrics for translation/summarization.
- **Human feedback**: Direct user/annotator assessment.
- **LLM-as-a-judge**: Models evaluate other models' outputs.

---

## üõ† Applications of LLMs

- ‚úÖ Code generation
- üõ† Code explanation & debugging
- üåç Machine translation
- üìù Summarization
- ü§ñ Chatbots and assistants
- üßæ Text classification (spam detection, sentiment, etc.)
- üîç Information retrieval and RAG
- üé® Multimodal tasks (text + image/audio/video)

---

## ‚úÖ Best Practices

- Keep prompts clear and goal-oriented
- Use few-shot examples for better guidance
- Control output format (e.g., JSON, CSV)
- Experiment with temperature and sampling
- Log prompt/output pairs for analysis
- Use validators to ensure structured output correctness
- Add reasoning steps for complex tasks (Chain-of-Thought)
- Collaborate, iterate, and document your findings

